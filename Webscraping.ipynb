{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3852cfc-8019-43f4-be72-0a663d21832e",
   "metadata": {},
   "source": [
    "Throughout the project, we do our analysis using data from RateMyProfessors and Reddit. Since the code is very long and tedious to webscrape, we split it into this notebook. This also helps because this code takes a while to run. All the data is outputed to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2203c7ff-3129-48a7-a329-43dd3f7a1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the entire project\n",
    "\n",
    "import requests\n",
    "import bs4   # also needs html5lib\n",
    "import string\n",
    "import pandas\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0850a-97eb-479e-a7a0-ae900ac9aa6f",
   "metadata": {},
   "source": [
    "The function below allows us to scrape data from both UVA and Virginia Tech's subreddits. The function's parameters are the name of the subreddit, the name of the .csv file we want it to produce, and how many posts we want to parse through. For this analysis, we have specified for 500 posts to be collected. The data is stored in \"VirginiaTechPosts.csv\" and \"UVAPosts.csv\".\n",
    "\n",
    "The following Youtube tutorial was used to help implement this functionality: https://www.youtube.com/watch?v=MpFvkptjekk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c714b13-5c0e-465a-878f-0d19a3616113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the reddit webscrapping API\n",
    "\n",
    "import praw\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9269d10-a7c6-4368-aa78-3ad38dbd3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subreddit_posts_to_csv(subreddit_name, filename=\"subreddit_posts.csv\", num_posts=100):\n",
    "\n",
    "    # Input your id that you aquire from the reddit.com\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=\"\",\n",
    "            client_secret=\"\",\n",
    "            user_agent=\"\",\n",
    "        )\n",
    "\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "        posts = []\n",
    "        start_time = time.time()\n",
    "        rate_limit = 100\n",
    "        for i, post in enumerate(subreddit.new(limit=num_posts)):\n",
    "            posts.append(post)\n",
    "\n",
    "            # Abiding by the crawl limit so we don't get booted\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if (i + 1) % rate_limit == 0:  # Check if we've reached the rate limit\n",
    "                if elapsed_time < 60:\n",
    "                    time.sleep(60 - elapsed_time)  # Wait for the remaining time\n",
    "                start_time = time.time()  # Reset the timer\n",
    "\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['title', 'url', 'selftext', 'score', 'num_comments', 'created_utc']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            for post in posts:\n",
    "                writer.writerow({\n",
    "                    'title': post.title,\n",
    "                    'url': post.url,\n",
    "                    'selftext': post.selftext,\n",
    "                    'score': post.score,\n",
    "                    'num_comments': post.num_comments,\n",
    "                    'created_utc': post.created_utc,\n",
    "                })\n",
    "\n",
    "        print(f\"Successfully saved {num_posts} posts to {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f866c-7c8e-4045-a263-3c77e96151a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes the Virginia Tech Reddit Posts\n",
    "subreddit_name = \"VirginiaTech\"\n",
    "scrape_subreddit_posts_to_csv(subreddit_name, filename=\"VirginiaTechPosts.csv\", num_posts=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fca6e9-6778-4b37-94ec-364608a05d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes the UVA Reddit posts\n",
    "subreddit_name = \"UVA\"\n",
    "scrape_subreddit_posts_to_csv(subreddit_name, filename=\"UVAPosts.csv\", num_posts=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a59f9d-a4d1-44b6-93d8-c19f3014540e",
   "metadata": {},
   "source": [
    "In an effort to gain more insight into the RateMyProfessor comments, we went ahead and extracted the overall rating attached to the comment. With this we are hoping to explore if there is a relationship between the sentiment of the comment and rating itself.\n",
    "\n",
    "Starting off, we used Selenium to scrap these reviews and attach them to the same comments we used in our inital analysis. This data is then stored in \"VTRMPNEW.csv\" and \"UVARMPNEW.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85e10e-2c73-4573-bda9-cbc22c2ba78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virginia Tech data\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Open the target RateMyProfessors page\n",
    "url = \"https://www.ratemyprofessors.com/school/1349\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load initially\n",
    "time.sleep(2)\n",
    "\n",
    "# List to store all the reviews\n",
    "reviews = []\n",
    "\n",
    "# Define a function to scrape reviews after clicking \"Show More\"\n",
    "def scrape_reviews():\n",
    "\n",
    "    # Find all review elements\n",
    "    rating_elements = driver.find_elements(By.CLASS_NAME, 'GradeSquare__ColoredSquare-sc-6d97x2-0')\n",
    "    review_elements = driver.find_elements(By.CLASS_NAME, 'SchoolRating__RatingComment-sb9dsm-6')\n",
    "\n",
    "    # Extract the text of each review\n",
    "    for rating, review in zip(rating_elements[12:], review_elements):\n",
    "        reviews.append((float(rating.text), review.text))\n",
    "\n",
    "\n",
    "\n",
    "# Try to click the \"Show More\" button and load more reviews\n",
    "# Loop to click the \"Show More\" button multiple times\n",
    "clicks = 10\n",
    "while clicks != 0:\n",
    "    try:\n",
    "        # Find the \"Show More\" button by its XPath\n",
    "        show_more_button = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div/div/main/div[3]/div[1]/button')\n",
    "\n",
    "        # Click the \"Show More\" button\n",
    "        show_more_button.click()\n",
    "\n",
    "        # Wait for the content to load after clicking\n",
    "        time.sleep(2)  # Adjust sleep time based on the page's loading speed\n",
    "\n",
    "\n",
    "        clicks -= 1\n",
    "\n",
    "    except Exception as e:\n",
    "        # If there's no more \"Show More\" button (end of the reviews), break the loop\n",
    "        print(\"No more 'Show More' button found or end of reviews.\")\n",
    "        break\n",
    "\n",
    "scrape_reviews()\n",
    "\n",
    "\n",
    "# Print the reviews collected\n",
    "for review in reviews:\n",
    "    print(review)\n",
    "\n",
    "\n",
    "# Close the browser window\n",
    "driver.quit()\n",
    "\n",
    "df_vtrmp_new = pandas.DataFrame(reviews, columns=[\"Rating\", \"Review\"])\n",
    "df_vtrmp_new.to_csv(\"VTRMPNEW.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0f68d-c486-43c3-be01-e6a7fb752b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UVA Data\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Open the target RateMyProfessors page\n",
    "url = \"https://www.ratemyprofessors.com/school/1277\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load initially\n",
    "time.sleep(2)\n",
    "\n",
    "# List to store all the reviews\n",
    "reviews = []\n",
    "\n",
    "# Define a function to scrape reviews after clicking \"Show More\"\n",
    "def scrape_reviews():\n",
    "\n",
    "    # Find all review elements\n",
    "    rating_elements = driver.find_elements(By.CLASS_NAME, 'GradeSquare__ColoredSquare-sc-6d97x2-0')\n",
    "    review_elements = driver.find_elements(By.CLASS_NAME, 'SchoolRating__RatingComment-sb9dsm-6')\n",
    "\n",
    "    # Extract the text of each review\n",
    "    for rating, review in zip(rating_elements[12:], review_elements):\n",
    "        reviews.append((float(rating.text), review.text))\n",
    "\n",
    "\n",
    "# Try to click the \"Show More\" button and load more reviews\n",
    "# Loop to click the \"Show More\" button multiple times\n",
    "clicks = 10\n",
    "while clicks != 0:\n",
    "    try:\n",
    "        # Find the \"Show More\" button by its XPath\n",
    "        show_more_button = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div/div/main/div[3]/div[1]/button')\n",
    "\n",
    "        # Click the \"Show More\" button\n",
    "        show_more_button.click()\n",
    "\n",
    "        # Wait for the content to load after clicking\n",
    "        time.sleep(2)  # Adjust sleep time based on the page's loading speed\n",
    "\n",
    "\n",
    "        clicks -= 1\n",
    "\n",
    "    except Exception as e:\n",
    "        # If there's no more \"Show More\" button (end of the reviews), break the loop\n",
    "        print(\"No more 'Show More' button found or end of reviews.\")\n",
    "        break\n",
    "\n",
    "scrape_reviews()\n",
    "\n",
    "\n",
    "# Print the reviews collected\n",
    "for review in reviews:\n",
    "    print(review)\n",
    "\n",
    "\n",
    "# Close the browser window\n",
    "driver.quit()\n",
    "\n",
    "df_uvarmp_new = pandas.DataFrame(reviews, columns=[\"Rating\", \"Review\"])\n",
    "df_uvarmp_new.to_csv(\"UVARMPNEW.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55ddf6-b71b-411a-89be-13b5b90380a7",
   "metadata": {},
   "source": [
    "Now we want to assess how the 10 different aspects of student life (Food, Facilities, Reputation, Happiness, Safety, Opportunities, Clubs, Social, Internet, Location) factor into these reviews. To do this, we're analyzing the RateMyProfessors comments by collecting the rating for each attribute associated with individual comments. We determine these ratings by recording the gray value of each of the 5 bar segments and using logic to identify filled versus unfilled bars. For example, if no bar segments are gray (unfilled), then the attribute has received a rating of 5.\n",
    "\n",
    "These results are then stored in CSV files named \"all_vt.csv\" and \"all_uva.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23427c81-90bd-4162-866a-709e9dfa9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4   # also needs html5lib\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "# Load the page\n",
    "driver.get(\"https://www.ratemyprofessors.com/school/1349\")\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "show_more_xpath = '//*[@id=\"root\"]/div/div/main/div[3]/div[1]/button'\n",
    "\n",
    "# Click \"Show More\" if available\n",
    "for iteration in range(10):\n",
    "    try:\n",
    "        # Wait until the button is clickable\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        btn = wait.until(EC.element_to_be_clickable((By.XPATH, show_more_xpath)))\n",
    "\n",
    "        driver.execute_script(\n",
    "            \"var iframe = document.getElementsByName('IL_SR_FRAME2');\"\n",
    "            \"if(iframe.length > 0){ iframe[0].remove(); }\"\n",
    "        )\n",
    "\n",
    "        # Scroll the button into view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", btn)\n",
    "        time.sleep(1)  # Allow time for scrolling\n",
    "\n",
    "        # Attempt to click the button\n",
    "        btn.click()\n",
    "        print(f\"Clicked 'Show More' button ({iteration+1}/10)\")\n",
    "        time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking 'Show More' at iteration {iteration+1}: {e}\")\n",
    "        try:\n",
    "            # Fallback: Use JavaScript click if normal click fails\n",
    "\n",
    "            btn = driver.find_element(By.XPATH, show_more_xpath)\n",
    "            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "            print(f\"JavaScript clicked 'Show More' button at iteration {iteration+1}\")\n",
    "            time.sleep(2)\n",
    "        except Exception as js_e:\n",
    "            print(f\"JavaScript click also failed at iteration {iteration+1}: {js_e}\")\n",
    "            break\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located(\n",
    "    (By.CSS_SELECTOR, \"div.DisplaySlider__DisplaySliderContainer-sc-6etfq5-0\")\n",
    "))\n",
    "\n",
    "# Extract updated HTML content\n",
    "html = driver.page_source\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2f39c-44d7-40df-a8ca-bfdeb3d72ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "comment_containers = soup.find_all('div', class_=\"SchoolRatingSummary__SchoolRatingSummaryContainer-sc-50tcmg-0\")\n",
    "\n",
    "all_comments_data = []\n",
    "\n",
    "for idx, comment in enumerate(comment_containers):\n",
    "    # For each comment container, find the 10 slider sections.\n",
    "    # Here, we assume each slider section has the class \"DisplaySlider__DisplaySliderContainer-sc-6etfq5-0\".\n",
    "    slider_sections = comment.find_all('div', class_=\"DisplaySlider__DisplaySliderContainer-sc-6etfq5-0\")\n",
    "\n",
    "    if len(slider_sections) != 10:\n",
    "        # Optionally, print a message if the expected number of slider sections is not found.\n",
    "        print(f\"Comment {idx}: Expected 10 slider sections but found {len(slider_sections)}. Skipping this comment.\")\n",
    "        continue  # or handle it differently if needed\n",
    "\n",
    "    # Dictionary to hold the data for the current comment.\n",
    "    comment_data = {}\n",
    "\n",
    "    for section in slider_sections:\n",
    "        data_dict = {}\n",
    "        label_element = section.find('div', class_=\"DisplaySlider__DisplaySliderLabel-sc-6etfq5-1\")\n",
    "        label = label_element.text.strip() if label_element else \"Unknown Label\"\n",
    "\n",
    "        slider_boxes = section.find_all('div', class_=\"DisplaySlider__DisplaySliderBox-sc-6etfq5-3\")\n",
    "\n",
    "        # Extract numerical values from bars (assuming there's relevant data inside)\n",
    "        values = [box.get('class')[-1] for box in slider_boxes]  # Modify extraction logic as needed\n",
    "\n",
    "        comment_data[label] = values if values else \"N/A\"\n",
    "\n",
    "    all_comments_data.append(comment_data)\n",
    "\n",
    "df = pd.DataFrame(all_comments_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a508c-507b-4853-ad2e-13af5751b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of gray levels in each bar\n",
    "def count_grays(color_list):\n",
    "    gray_values = {\"gytHZE\", \"guPMgv\", \"dDIWaC\"} # Set of known gray values\n",
    "    return 5 - sum(1 for color in color_list if color in gray_values) # Returns the level for each bar\n",
    "\n",
    "df_gray_counts = df.applymap(count_grays) # Gets the level of each datapoint in the frame\n",
    "\n",
    "df_gray_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68001209-6ed6-4311-8854-65827f0c473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat along columns (axis=1)\n",
    "result = pd.concat([vt_data, df_gray_counts], axis=1)\n",
    "result.to_csv(\"all_vt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1de2b-964f-406d-9ad6-6f51e73ee452",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "# Load the page\n",
    "driver.get(\"https://www.ratemyprofessors.com/school/1277\")\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "show_more_xpath = '//*[@id=\"root\"]/div/div/main/div[3]/div[1]/button'\n",
    "\n",
    "# Click \"Show More\" if available\n",
    "for iteration in range(10):\n",
    "    try:\n",
    "        # Wait until the button is clickable\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        btn = wait.until(EC.element_to_be_clickable((By.XPATH, show_more_xpath)))\n",
    "\n",
    "        driver.execute_script(\n",
    "            \"var iframe = document.getElementsByName('IL_SR_FRAME2');\"\n",
    "            \"if(iframe.length > 0){ iframe[0].remove(); }\"\n",
    "        )\n",
    "\n",
    "        # Scroll the button into view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", btn)\n",
    "        time.sleep(1)  # Allow time for scrolling\n",
    "\n",
    "        # Attempt to click the button\n",
    "        btn.click()\n",
    "        print(f\"Clicked 'Show More' button ({iteration+1}/10)\")\n",
    "        time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking 'Show More' at iteration {iteration+1}: {e}\")\n",
    "        try:\n",
    "            # Fallback: Use JavaScript click if normal click fails\n",
    "\n",
    "            btn = driver.find_element(By.XPATH, show_more_xpath)\n",
    "            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "            print(f\"JavaScript clicked 'Show More' button at iteration {iteration+1}\")\n",
    "            time.sleep(2)\n",
    "        except Exception as js_e:\n",
    "            print(f\"JavaScript click also failed at iteration {iteration+1}: {js_e}\")\n",
    "            break\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located(\n",
    "    (By.CSS_SELECTOR, \"div.DisplaySlider__DisplaySliderContainer-sc-6etfq5-0\")\n",
    "))\n",
    "\n",
    "# Extract updated HTML content\n",
    "html = driver.page_source\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133e934-368f-488e-bb63-ab509119a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup = bs4.BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "comment_containers = soup.find_all('div', class_=\"SchoolRatingSummary__SchoolRatingSummaryContainer-sc-50tcmg-0\")\n",
    "\n",
    "all_comments_data = []\n",
    "\n",
    "for idx, comment in enumerate(comment_containers):\n",
    "    # For each comment container, find the 10 slider sections.\n",
    "    # Here, we assume each slider section has the class \"DisplaySlider__DisplaySliderContainer-sc-6etfq5-0\".\n",
    "    slider_sections = comment.find_all('div', class_=\"DisplaySlider__DisplaySliderContainer-sc-6etfq5-0\")\n",
    "\n",
    "    if len(slider_sections) != 10:\n",
    "        # Optionally, print a message if the expected number of slider sections is not found.\n",
    "        print(f\"Comment {idx}: Expected 10 slider sections but found {len(slider_sections)}. Skipping this comment.\")\n",
    "        continue  # or handle it differently if needed\n",
    "\n",
    "    # Dictionary to hold the data for the current comment.\n",
    "    comment_data = {}\n",
    "\n",
    "    for section in slider_sections:\n",
    "        data_dict = {}\n",
    "        label_element = section.find('div', class_=\"DisplaySlider__DisplaySliderLabel-sc-6etfq5-1\")\n",
    "        label = label_element.text.strip() if label_element else \"Unknown Label\"\n",
    "\n",
    "        slider_boxes = section.find_all('div', class_=\"DisplaySlider__DisplaySliderBox-sc-6etfq5-3\")\n",
    "\n",
    "        # Extract numerical values from bars (assuming there's relevant data inside)\n",
    "        values = [box.get('class')[-1] for box in slider_boxes]  # Modify extraction logic as needed\n",
    "\n",
    "        comment_data[label] = values if values else \"N/A\"\n",
    "\n",
    "    all_comments_data.append(comment_data)\n",
    "\n",
    "df = pd.DataFrame(all_comments_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ff019-05fa-4f8e-8deb-80a9b8031f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gray_counts = df.applymap(count_grays) # Gets the level of each datapoint in the frame\n",
    "\n",
    "df_gray_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e3b31-2912-4fd8-bb8c-2c336bb4aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat along columns (axis=1)\n",
    "result = pd.concat([uva_data, df_gray_counts], axis=1)\n",
    "result.to_csv(\"all_uva.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396fabb-2bfc-43df-9e6f-1c34c85e99f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
